services:
  # FastMCP服务器 - 暴露工具接口
  mcp-server:
    build:
      context: .
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: fastmcp-demo:latest  # 使用共享镜像标签
    ports:
      - "8100:8100"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
    command: /app/.venv/bin/python mcp_server.py
    restart: unless-stopped
    networks:
      - fastmcp-network

  # FastAPI Chat服务器 - 提供聊天服务
  chat-server:
    image: fastmcp-demo:latest  # 复用同一个镜像，避免重复构建
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - MCP_SERVER_URL=http://mcp-server:8100
      # 模型路径：从.env文件读取，或使用默认值（仅支持Llama 3.1 8B）
      - LLAMA_MODEL_PATH=${LLAMA_MODEL_PATH:-./models/llama-3.1-8b-instruct-q4_k_m.gguf}
    command: /app/.venv/bin/python chat_server.py
    depends_on:
      - mcp-server
    restart: unless-stopped
    networks:
      - fastmcp-network

networks:
  fastmcp-network:
    driver: bridge
