# FastMCP Minimal Example

This is a minimal example based on FastMCP, demonstrating how to build a complete tool calling system:
- **FastMCP Server**: Exposes calculation tool interfaces (via MCP protocol and SSE transport)
- **FastAPI Chat Server**: Provides chat service, LLM calls FastMCP tools through MCP client

<img width="3840" height="2110" alt="b076f65573ef5b15190df9424cd20a12" src="https://github.com/user-attachments/assets/30232728-9b46-4393-8a54-8b09b26f685b" />

## Related Projects:

- https://github.com/fastapi/fastapi
- https://github.com/jlowin/fastmcp
- https://github.com/ggml-org/llama.cpp
- https://github.com/run-llama/llama_index

## Model Used

- https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF

## Reference Documentation

- https://developers.llamaindex.ai/python/examples/agent/react_agent/
- https://www.ibm.com/think/topics/react-agent
- https://modelcontextprotocol.io/docs/getting-started/intro
- https://hackteam.io/blog/your-llm-does-not-care-about-mcp/
- https://medium.com/@infin94/kickstart-your-research-instantly-generate-synthetic-text-data-with-llama-3-1-56eaee6fbf48
- https://levelup.gitconnected.com/how-i-built-a-tool-calling-llama-agent-with-a-custom-mcp-server-3bc057d27e85
- https://www.anthropic.com/news/model-context-protocol
- https://github.com/Kludex/starlette

## Features

- ğŸ¤– **Real LLM Inference**: Uses Llama 3.1 8B language model (supports native tool_calls)
- ğŸ’¬ **Friendly Conversation**: Supports natural language dialogue, can friendly reply to greetings and casual chat
- ğŸ› ï¸ **Intelligent Tool Calling**: Uses LlamaIndex ReActAgent to automatically handle tool calls, LLM calls FastMCP server tools through MCP protocol
- ğŸ”Œ **FastMCP Integration**: Uses FastMCP framework to expose tools, provides tool interfaces through SSE protocol
- ğŸ³ Docker Containerized Deployment: Supports multi-service architecture (FastMCP server + Chat server)
- ğŸŒ HTTP API interface, supports curl interaction
- âš¡ CPU inference based on llama.cpp
- ğŸ›¡ï¸ Comprehensive error handling and friendly error messages

## Quick Start

### 1. Download Model

The project uses **Llama 3.1 8B-Instruct** model (supports native tool_calls).

**Characteristics**:
- Model size: ~4.6GB
- Memory requirement: ~8GB RAM
- Tool calling: Native tool_calls support, automatically handled by LlamaIndex
- Inference speed: Medium
- **Advantage**: More accurate and reliable tool calling, better context understanding

**Download Methods**:

**Important**: Meta official version (`meta-llama/Llama-3.1-8B-Instruct`) requires login authentication.  
**Recommended**: Use community public quantized version (no authentication required, same functionality):

```bash
# Method 1: Direct download with wget (recommended, simplest)
mkdir -p models
# Download from bartowski (public version, no authentication, ~4.6GB)
wget -O models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf \
  "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# Note: Filename must be Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf, otherwise service cannot start

# Method 2: Use huggingface-cli (bartowski version, no login required)
pip install huggingface_hub
huggingface-cli download bartowski/Meta-Llama-3.1-8B-Instruct-GGUF \
  --include "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --local-dir ./models
```

**Alternative Public Sources** (all require no authentication, sorted by download count):
- `bartowski/Meta-Llama-3.1-8B-Instruct-GGUF` (95k+ downloads, recommended)
- `MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF` (76.2k downloads)
- `QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF` (55.5k downloads)

**Notes**:
- File size is ~4.6GB, ensure sufficient disk space
- Model filename is fixed as: `Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`
- File must be placed in `./models/` directory

### 2. Build and Start

#### Method 1: Use Build Script (Recommended)
```bash
# 1. Configure proxy (optional)
cp env.example .env
# Edit .env file, set your proxy configuration

# 2. Use build script
./build.sh

# 3. Start services
docker-compose up
```

#### Method 2: Manual Build
```bash
# No proxy environment
docker-compose build
docker-compose up

# Enterprise proxy environment
export PROXY_URL=http://your-proxy:port
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
export NO_PROXY=localhost,127.0.0.1

docker-compose build --build-arg proxy_url=$PROXY_URL --build-arg http_proxy=$HTTP_PROXY --build-arg https_proxy=$HTTPS_PROXY --build-arg no_proxy=$NO_PROXY
docker-compose up
```

#### Configuration Notes
- **PROXY_URL**: Proxy server address (e.g., `http://proxy.company.com:8080`)
- **HTTP_PROXY/HTTPS_PROXY**: Proxy settings during Docker build
- **NO_PROXY**: List of addresses that should not use proxy

**Note**: The project includes an automatic proxy configuration script `install.sh`, which automatically handles proxy settings inside containers based on environment variables.

Services will start at the following addresses:
- **FastMCP Server**: `http://localhost:8100` (provides tool interfaces)
- **Chat Server**: `http://localhost:8000` (provides chat service)

**Startup Verification**:
After startup, check logs, you should see:
- `Loading model: ./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`
- `Model loaded successfully`
- `MCP server connected successfully, found 3 tools`
- `Agent initialization complete, tool calls will be automatically handled by LlamaIndex`

**Runtime Logs**:
- LlamaIndex will automatically handle tool calls, logs will show tool call process
- Using `verbose=True` you can see detailed tool call and response information

**Notes**:
- Need to download Llama 3.1 8B model file first (~4.6GB)
- Tool calls are automatically handled by LlamaIndex ReActAgent, no manual parsing needed
- Supports native tool_calls, no text parsing needed
- Agent maximum iteration count set to 3, to avoid excessive response time
- Response contains raw complete output (`raw_response`)

### 3. Test API

#### Health Check
```bash
curl http://localhost:8000/health
```
**Expected Output**:
```json
{"status":"healthy","agent_loaded":true,"mcp_available":true,"tools_count":3}
```

**Note**: If `agent_loaded` is `false`, it means the model file was not found or Agent initialization failed, need to download model file first.

#### View Available Tools

**Note**: FastMCP server uses SSE protocol (`/sse` endpoint), cannot be accessed directly via curl. Tool list is obtained through HTTP API provided by Chat server.

**Get tool list from Chat server**:
```bash
curl http://localhost:8000/tools
```

#### Chat Test

**Notes**: 
1. If Chinese displays as Unicode escape characters (like `\u6211`), you can use `jq` or `python3 -m json.tool` to display correctly
2. Please ensure to use **English quotes**, not Chinese quotes ("")

```bash
# Greeting conversation (natural language reply)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "ä½ å¥½"}' | jq .
# Expected output:
# {
#   "raw_response": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }

# Simple addition (tool call)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "è®¡ç®— 5 + 3"}' | jq .
# Expected output:
# {
#   "raw_response": "Thought: ... Answer: 8 ...",  # Complete Agent output
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }

# Multiplication (tool call)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "è®¡ç®— 4 * 7"}' | jq .
# Expected output:
# {
#   "raw_response": "Calculation result: 28", 
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }

# Expression calculation (tool call)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "è®¡ç®— 2+3*4"}' | jq .
# Expected output:
# {
#   "raw_response": "Calculation result: 14",
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }

# Complex expression (tool call)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "è®¡ç®— 3 * 7"}' | jq .
# Expected output:
# {
#   "raw_response": "Calculation result: 21",
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }

# Non-calculation message (natural language reply)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "ä»Šå¤©å¤©æ°”å¦‚ä½•"}' | jq .
# Expected output:
# {
#   "raw_response": "ä»Šå¤©çš„å¤©æ°”å–å†³äºä½ æ‰€åœ¨çš„åœ°æ–¹ï¼Œä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ åœ¨å“ªé‡Œå—ï¼Ÿ",
#   "tools_available": ["add_numbers", "multiply_numbers", "calculate_expression"]
# }
```

**Feature Description**:
- ğŸ§® **Calculation Requests**: When users ask mathematical calculation questions, LLM will automatically call corresponding tools for calculation
- ğŸ’¬ **Friendly Conversation**: When users greet or chat casually, LLM will reply friendly in natural language (will not call tools)
- ğŸ” **Intelligent Recognition**: LLM will automatically recognize user intent, decide whether to use tools or reply directly
- âš¡ **Fast Response**: Maximum iteration count limited to 3, ensuring reasonable response time
- ğŸ“ **Raw Response**: Returns complete raw output (`raw_response`)

**Alternative** (if system doesn't have `jq` installed):
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "è®¡ç®— 5 + 3"}' | python3 -m json.tool
```

## Project Architecture

### ğŸ” Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User Request (curl)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FastAPI Chat Server (Port 8000)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LlamaIndex + llama-cpp-python â†’ Llama 3.1 8B        â”‚   â”‚
â”‚  â”‚  - Analyze user request                              â”‚   â”‚
â”‚  â”‚  - Generate tool call declaration                    â”‚   â”‚
â”‚  â”‚  - Generate final response based on tool result      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â”‚ SSE Connection                 â”‚
â”‚                            â”‚ (via MCP Client)               â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  MCP Client (BasicMCPClient)                         â”‚   â”‚
â”‚  â”‚  - Get tool list                                     â”‚   â”‚
â”‚  â”‚  - Call tool interface                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ SSE Protocol (/sse)
                            â”‚ MCP Protocol over SSE
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastMCP Server (Port 8100)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FastMCP Framework                                   â”‚   â”‚
â”‚  â”‚  - @mcp.tool() decorator registers tools             â”‚   â”‚
â”‚  â”‚  - SSE transport protocol (/sse)                     â”‚   â”‚
â”‚  â”‚  - Auto-exposes tools via MCP protocol               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tool Execution Layer                                â”‚   â”‚
â”‚  â”‚  - add_numbers()                                     â”‚   â”‚
â”‚  â”‚  - multiply_numbers()                                â”‚   â”‚
â”‚  â”‚  - calculate_expression()                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points**:
- **FastMCP Server** (port 8100): Registers tools through `@mcp.tool()` decorator, provides SSE endpoint (`/sse`) to expose MCP protocol interface
- **FastAPI Chat Server** (port 8000): Uses llama-cpp-python for LLM inference, connects to FastMCP server's SSE endpoint through MCP client (BasicMCPClient)
- **Workflow**: User request â†’ Chat server â†’ LLM analysis â†’ MCP protocol (SSE) calls FastMCP tools â†’ Returns result â†’ LLM generates final reply

### ğŸš€ Data Flow Example

**User Request: "Calculate 25 + 17"**

```
1. User â†’ Chat Server (POST /chat)
   {"message": "Calculate 25 + 17"}

2. Chat Server â†’ LLM (Round 1 inference)
   LLM analysis: "Need to call add_numbers tool"
   Generate tool call declaration: [add_numbers(a=25, b=17)]

3. Chat Server â†’ FastMCP Server (via SSE /sse endpoint)
   MCP protocol call: add_numbers(a=25, b=17)

4. FastMCP Server â†’ Tool Execution
   Execute: add(25, 17) â†’ return 42

5. FastMCP Server â†’ Chat Server
   Return: {"result": 42}

6. Chat Server â†’ LLM (Round 2 inference)
   Feed tool result to LLM, LLM generates final response

7. Chat Server â†’ User
   {"response": "Calculation result: 42", "tools_available": [...]}
```

## Project Structure

```
fastmcp_demo/
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose configuration (two services: mcp-server + chat-server)
â”œâ”€â”€ pyproject.toml          # Python project configuration and dependencies (managed with uv)
â”œâ”€â”€ install.sh             # Automatic installation script (handles proxy configuration)
â”œâ”€â”€ build.sh               # Build script (supports environment variable configuration)
â”œâ”€â”€ start_servers.sh       # Local startup script (starts both services simultaneously)
â”œâ”€â”€ env.example            # Environment configuration example file
â”œâ”€â”€ mcp_server.py          # FastMCP server (port 8100)
â”œâ”€â”€ chat_server.py         # FastAPI Chat server (port 8000)
â”œâ”€â”€ models/                # Model file directory (Volume mount)
â””â”€â”€ README.md              # Usage instructions
```

## Technology Stack

- **FastMCP Framework**: FastMCP >= 0.1.0 (MCP protocol implementation)
- **MCP Protocol**: Model Context Protocol (tool calling protocol)
- **LlamaIndex**: >= 0.10.0 (Agent framework that automatically handles tool calls)
- **AI Model**: Llama 3.1 8B-Instruct-GGUF (supports native tool_calls)
- **Inference Engine**: llama-cpp-python (based on llama.cpp)
- **LLM Library**: llama-cpp-python >= 0.2.0
- **Web Framework**: FastAPI >= 0.104.0 (Chat server)
- **HTTP Client**: httpx >= 0.25.0 (MCP client)
- **ASGI Server**: uvicorn >= 0.24.0
- **Containerization**: Docker + Docker Compose
- **Proxy Handling**: Automatic proxy configuration script

## Model Information

### Llama 3.1 8B-Instruct (Default, Recommended)

- **Parameters**: 8B
- **Quantization**: Q4_K_M (~4.6GB)
- **Memory Requirement**: ~8GB RAM
- **Tool Calling**: **Stronger tool calling capability** (more accurate and reliable tool calling)
- **Inference**: CPU inference, no GPU required
- **Speed**: Medium (but tool calling is more reliable)
- **Advantage**: Better tool calling capability, more accurate and reliable


## Running Modes

The project **defaults to real LLM mode**, requires downloading model files to run. Model files will be automatically loaded at startup and perform real inference calculations.

### Real LLM Mode (Default)
The project uses real Llama 3.1 8B model for inference:
- âœ… **Real LLM Inference**: Uses llama-cpp-python to actually call the model
- âœ… **Intelligent Tool Calling**: Uses LlamaIndex ReActAgent to automatically handle tool calls
- âœ… **Native tool_calls Support**: Llama 3.1 8B supports native tool_calls, no text parsing needed
- âœ… **Friendly Conversation**: Supports natural language dialogue, can friendly reply to greetings and casual chat
- âœ… **Error Handling**: Comprehensive parameter validation and error messages
- âš ï¸ **Requires Model File**: Must download model file to `./models/` directory to run

If model file does not exist, the service will fail to start and display error message.

## Important Notes

1. **Model File Required**: Must download Llama 3.1 8B model file (~4.6GB) to `./models/` directory, otherwise service cannot start
2. **Memory Requirement**: Recommend at least 8GB available memory (model requires ~8GB RAM)
3. **Network Connection**: First-time model download requires good network connection
4. **Proxy Environment**: Enterprise network environments need to configure proxy, see build instructions for details
5. **Request Format**: When using curl, ensure JSON uses English quotes, e.g., `'{"message": "ä½ å¥½"}'`
6. **Tool Calling**: Tool calls are automatically handled by LlamaIndex, no manual parsing or configuration needed

## LLM Usage Principles

### 1. Architecture Overview

The project adopts a **dual-server architecture**:
- **FastMCP Server** (`mcp_server.py`): Registers tools through `@mcp.tool()` decorator, exposes MCP protocol interface through SSE protocol (`/sse` endpoint)
- **Chat Server** (`chat_server.py`): Uses llama-cpp-python for LLM inference, connects to FastMCP server's SSE endpoint through MCP client (BasicMCPClient)

### 2. FastMCP Server

```python
# Create FastMCP instance
mcp = FastMCP("MathTools")

# Register tools
@mcp.tool()
def add_numbers(a: float, b: float) -> float:
    """Add two numbers"""
    return add(a, b)

# Start server (via SSE transport protocol)
# After FastMCP tools are registered, MCP protocol interface is automatically exposed through SSE endpoint (/sse)
mcp.run(transport="sse")
```

**FastMCP Server Features**:
- Registers tools through `@mcp.tool()` decorator (`add_numbers`, `multiply_numbers`, `calculate_expression`)
- Provides MCP protocol interface using SSE transport protocol:
  - SSE endpoint: `http://0.0.0.0:8100/sse`
  - Tool list and calls are automatically exposed through MCP protocol
- FastMCP framework automatically handles tool registration and calls through SSE protocol

### 3. Chat Server and LLM Inference (Automatically Handled by LlamaIndex)

#### a) Model Loading and Agent Initialization
```python
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.tools.mcp import McpToolSpec, BasicMCPClient
from llama_index.core.agent import ReActAgent

# Load Llama model
llm = LlamaCPP(
    model_path=model_path,  # Llama 3.1 8B GGUF file
    temperature=0.1,
    max_new_tokens=256,
    context_window=4096,
    model_kwargs={"n_threads": 6},
)

# Connect to FastMCP server to get tools (via SSE endpoint)
mcp_sse_url = "http://localhost:8100/sse"
client = BasicMCPClient(command_or_url=mcp_sse_url, timeout=10)
tool_spec = McpToolSpec(client=client)
tools = await tool_spec.to_tool_list_async()  # Async get tool list

# Create ReActAgent (automatically handles tool calls)
agent = ReActAgent.from_tools(
    tools=tools,
    llm=llm,
    verbose=True,
    system_prompt="You are a friendly math calculation assistant..."
)
```

#### b) Tool Call Flow (Automatically Handled)

**LlamaIndex ReActAgent automatically handles all tool calls**:

```python
# User request
handler = agent.run(
    user_msg="Calculate 25 + 17",
    memory=ChatMemoryBuffer(token_limit=3000),
    ctx=Context(agent),
    max_iterations=3  # Maximum iteration count, avoid excessive response time
)
result = await handler

# LlamaIndex will automatically:
# 1. Analyze user request
# 2. Decide if tool call is needed
# 3. If needed, generate tool_calls (native format)
# 4. Execute tool call
# 5. Feed result back to LLM
# 6. Generate final reply
```

**Advantages**:
- âœ… No manual parsing of tool calls needed
- âœ… Automatically handles native tool_calls format
- âœ… Supports multiple rounds of tool calls
- âœ… Comprehensive error handling and retry mechanism
- âœ… Raw response: Returns complete raw output
- âœ… Iteration count limit: Maximum 3 iterations, avoid long wait times

### 4. Complete Workflow Example

When user sends `"Calculate 25 + 17"`:

1. **User Request** â†’ Chat Server (`POST /chat`)
2. **LlamaIndex Agent Analysis**: ReActAgent automatically decides tool call is needed
3. **Generate tool_calls**: Agent generates native format tool_calls (`add_numbers(a=25, b=17)`)
4. **Automatically Execute Tool**: LlamaIndex calls FastMCP tool through MCP protocol
5. **FastMCP Executes Tool**: Calls `add_numbers(25, 17)` â†’ Returns `42`
6. **Generate Final Reply**: Agent feeds result back to LLM, generates friendly reply
7. **Return Final Result**: `"Calculation result: 42"`

### 5. Why Use LlamaIndex?

**Important Note**: Uses LlamaIndex framework to automatically handle tool calls, no manual parsing needed.

#### LlamaIndex Advantages

1. **Automatic Tool Call Handling**
   - âœ… Automatically recognizes when tool call is needed
   - âœ… Automatically handles native tool_calls format
   - âœ… No manual parsing of text or JSON needed

2. **Comprehensive Agent Architecture**
   - âœ… ReActAgent implements think-act-observe loop
   - âœ… Supports multiple rounds of tool calls
   - âœ… Automatically handles tool execution results

3. **Deep Integration with MCP Protocol**
   - âœ… `McpToolSpec` automatically gets tools from FastMCP server
   - âœ… Supports SSE protocol communication
   - âœ… Automatically converts tool formats

#### Key Features

- **MCP Protocol**: Uses FastMCP framework standardized tool calling method
- **LlamaIndex Integration**: Automatically handles all tool calling logic through LlamaIndex
- **Dual-Server Architecture**: Tool server and chat server separated, clear responsibilities
- **Native tool_calls Support**: Llama 3.1 8B supports native tool_calls, automatically handled by LlamaIndex
- **Local Inference**: Model runs completely locally, no network needed (except initial download)
- **CPU Optimized**: Uses llama.cpp for efficient CPU inference, no GPU needed

## Troubleshooting

### Proxy Related Issues
If you encounter connection issues in enterprise network environments:

1. **Use Configuration File** (Recommended):
   ```bash
   cp env.example .env
   # Edit .env file, set correct proxy address
   ./build.sh
   ```

2. **Manually Set Environment Variables**:
   ```bash
   export PROXY_URL=http://your-proxy:port
   export HTTP_PROXY=http://your-proxy:port
   export HTTPS_PROXY=http://your-proxy:port
   ./build.sh
   ```

3. **Check Proxy Connectivity**:
   ```bash
   curl -I --proxy $PROXY_URL https://pypi.org
   ```

4. **Rebuild**:
   ```bash
   docker-compose build --no-cache --build-arg proxy_url=$PROXY_URL --build-arg http_proxy=$HTTP_PROXY --build-arg https_proxy=$HTTPS_PROXY
   ```

### Model File Not Found
If service startup fails, prompting model file not found:
1. Ensure model file is downloaded to `./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`
2. Check if file path is correct (relative path is `./models/`)
3. Verify file permissions, ensure readable
4. Check server logs for detailed error information

### Insufficient Memory
If encountering insufficient memory, you can try:
- Reduce `n_ctx` parameter (in chat_server.py, default 4096)
- Reduce `n_threads` parameter (in chat_server.py, default 6)
- Use smaller quantized version model (like Q2_K or Q3_K_M)
- Close other memory-consuming programs

### Timeout and Performance Tuning
If encountering timeout errors ("Agent processing timeout"), you can adjust the following parameters based on hardware configuration:

#### 1. è°ƒæ•´ Token ç”Ÿæˆå‚æ•°ï¼ˆchat_server.pyï¼‰

**ä½ç½®**ï¼š`chat_server.py` ç¬¬ 79-90 è¡Œ

```python
llm = LlamaCPP(
    model_path=model_path,
    temperature=0.1,
    max_new_tokens=256,  # å¯è°ƒæ•´ï¼š128-512ï¼Œæ•°å€¼è¶Šå¤§ç”Ÿæˆå†…å®¹è¶Šå¤šï¼Œä½†è€—æ—¶æ›´é•¿
    context_window=4096,
    verbose=False,
    model_kwargs={
        "n_threads": 6,      # å¯è°ƒæ•´ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®ï¼Œå»ºè®®ä¸ºç‰©ç†æ ¸å¿ƒæ•°
        "n_predict": 256,    # åº”ä¸ max_new_tokens ä¿æŒä¸€è‡´
    },
)
```

**è°ƒæ•´å»ºè®®**ï¼š
- **å¿«é€Ÿå“åº”**ï¼ˆè¾ƒä½CPUï¼‰ï¼š`max_new_tokens=128`, `n_predict=128`
- **å¹³è¡¡**ï¼ˆä¸­ç­‰CPUï¼‰ï¼š`max_new_tokens=256`, `n_predict=256`ï¼ˆé»˜è®¤ï¼‰
- **å®Œæ•´å›å¤**ï¼ˆé«˜æ€§èƒ½CPUï¼‰ï¼š`max_new_tokens=512`, `n_predict=512`

#### 2. è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆchat_server.pyï¼‰

**ä½ç½®**ï¼š`chat_server.py` ç¬¬ 299 è¡Œ

```python
result = await asyncio.wait_for(handler, timeout=120.0)  # å¯è°ƒæ•´ï¼š60-300ç§’
```

**è°ƒæ•´å»ºè®®**ï¼š
- **å¿«é€Ÿç¡¬ä»¶**ï¼ˆ8æ ¸+CPUï¼Œé«˜é¢‘ç‡ï¼‰ï¼š60-90ç§’
- **ä¸­ç­‰ç¡¬ä»¶**ï¼ˆ4-6æ ¸CPUï¼‰ï¼š120ç§’ï¼ˆé»˜è®¤ï¼‰
- **è¾ƒæ…¢ç¡¬ä»¶**ï¼ˆ2-4æ ¸CPUï¼Œä½é¢‘ç‡ï¼‰ï¼š180-300ç§’

**è®¡ç®—å…¬å¼**ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰ï¼š
```
è¶…æ—¶æ—¶é—´ â‰ˆ (max_new_tokens / 10) + å·¥å…·è°ƒç”¨æ—¶é—´ï¼ˆ5-10ç§’ï¼‰
```

ä¾‹å¦‚ï¼š`max_new_tokens=256` â†’ è¶…æ—¶æ—¶é—´ â‰ˆ 25-35ç§’ + 5-10ç§’ â‰ˆ 30-45ç§’ï¼ˆå®é™…å»ºè®®è®¾ç½®ä¸º 2-3 å€ï¼Œå³ 60-120 ç§’ï¼‰

#### 3. è°ƒæ•´ Agent è¿­ä»£æ¬¡æ•°ï¼ˆchat_server.pyï¼‰

**ä½ç½®**ï¼š`chat_server.py` ç¬¬ 294 è¡Œ

```python
max_iterations=3  # å¯è°ƒæ•´ï¼š1-5ï¼Œç®€å•è®¡ç®—é€šå¸¸åªéœ€è¦1æ¬¡è¿­ä»£
```

**è°ƒæ•´å»ºè®®**ï¼š
- **ç®€å•è®¡ç®—**ï¼š`max_iterations=1-2`ï¼ˆæ›´å¿«å“åº”ï¼‰
- **å¤æ‚é—®é¢˜**ï¼š`max_iterations=3-5`ï¼ˆå…è®¸å¤šæ¬¡å·¥å…·è°ƒç”¨ï¼‰

#### 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®

**æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©å‚æ•°ç»„åˆ**ï¼š

| CPU æ ¸å¿ƒæ•° | æ¨è max_new_tokens | æ¨è timeout | æ¨è n_threads |
|-----------|-------------------|--------------|---------------|
| 2-4 æ ¸    | 128               | 180-240 ç§’   | 2-4           |
| 4-6 æ ¸    | 256               | 120-180 ç§’   | 4-6           |
| 6-8 æ ¸    | 256-512           | 90-120 ç§’    | 6-8           |
| 8+ æ ¸     | 512               | 60-90 ç§’     | 8+            |

**æ³¨æ„**ï¼š
- å‚æ•°è°ƒæ•´åéœ€è¦é‡å¯æœåŠ¡æ‰èƒ½ç”Ÿæ•ˆ
- å¦‚æœé¢‘ç¹è¶…æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘å‡å°‘ `max_new_tokens` è€Œä¸æ˜¯å¢åŠ  `timeout`
- CPU æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œè€ƒè™‘ä½¿ç”¨ GPU åŠ é€Ÿå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½

### System Prompt é…ç½®è¯´æ˜
å¦‚æœé‡åˆ° Agent è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸï¼ˆå¦‚é¢‘ç¹è°ƒç”¨å·¥å…·ã€å¾ªç¯è°ƒç”¨ç­‰ï¼‰ï¼Œå¯ä»¥è°ƒæ•´æˆ–ç§»é™¤ system_promptï¼š

#### System Prompt çš„ä½œç”¨

**ä½ç½®**ï¼š`chat_server.py` ç¬¬ 138-166 è¡Œ

å½“å‰ system_prompt çš„ä¸»è¦ä½œç”¨ï¼š
1. **é™åˆ¶å·¥å…·è°ƒç”¨æ¬¡æ•°**ï¼šæ˜ç¡®æŒ‡ç¤º"æ¯æ¬¡è¯·æ±‚æœ€å¤šåªè°ƒç”¨ä¸€æ¬¡å·¥å…·"ï¼Œé¿å…å¾ªç¯è°ƒç”¨
2. **æ˜ç¡®ä½¿ç”¨åœºæ™¯**ï¼šåªåœ¨æ•°å­¦è®¡ç®—æ—¶ä½¿ç”¨å·¥å…·ï¼Œé—®å€™å’Œé—²èŠä¸ä½¿ç”¨å·¥å…·
3. **æŒ‡å¯¼ Agent è¡Œä¸º**ï¼šæä¾›æ¸…æ™°çš„å·¥ä½œæµç¨‹å’Œç¤ºä¾‹

#### æ˜¯å¦å¯ä»¥å»æ‰ System Promptï¼Ÿ

**æ˜¯çš„ï¼ŒæŠ€æœ¯ä¸Šå¯ä»¥å»æ‰ã€‚**

ReActAgent å¯ä»¥åœ¨æ²¡æœ‰ `system_prompt` çš„æƒ…å†µä¸‹æ­£å¸¸å·¥ä½œï¼š

```python
agent = ReActAgent(
    tools=tools if tools else None,
    llm=llm,
    verbose=True,
    # system_prompt=system_prompt  # å¯ä»¥æ³¨é‡Šæ‰æˆ–åˆ é™¤
)
```

æˆ–è€…è®¾ç½®ä¸º `None`ï¼š

```python
agent = ReActAgent(
    tools=tools if tools else None,
    llm=llm,
    verbose=True,
    system_prompt=None  # æ˜ç¡®è®¾ç½®ä¸º None
)
```

#### å»æ‰ System Prompt çš„å½±å“

**ä»ç„¶å¯ä»¥å·¥ä½œ**ï¼š
- âœ… ReActAgent ä¼šä½¿ç”¨é»˜è®¤çš„ system prompt
- âœ… å·¥å…·è°ƒç”¨åŠŸèƒ½æ­£å¸¸
- âœ… åŸºæœ¬æ¨ç†èƒ½åŠ›ä¸å—å½±å“

**ä½†ä¼šæœ‰è¡Œä¸ºå·®å¼‚**ï¼š
- âš ï¸ æ²¡æœ‰æ˜ç¡®çš„å·¥å…·è°ƒç”¨é™åˆ¶ï¼ŒAgent å¯èƒ½ä¼šå¤šæ¬¡è°ƒç”¨å·¥å…·ï¼ˆå¯èƒ½å‡ºç°ä¹‹å‰é‡åˆ°çš„å¾ªç¯è°ƒç”¨é—®é¢˜ï¼‰
- âš ï¸ æ²¡æœ‰"åªè°ƒç”¨ä¸€æ¬¡å·¥å…·"çš„æ˜ç¡®æŒ‡å¯¼
- âš ï¸ æ²¡æœ‰é’ˆå¯¹æ•°å­¦è®¡ç®—çš„ä¸“é—¨æŒ‡å¯¼ï¼Œå¯èƒ½å¯¹ä»»ä½•é—®é¢˜éƒ½å°è¯•è°ƒç”¨å·¥å…·
- âš ï¸ å¯¹äºé—®å€™å’Œé—²èŠä¹Ÿå¯èƒ½å°è¯•è°ƒç”¨å·¥å…·

#### ä½•æ—¶éœ€è¦ä¿ç•™ System Promptï¼Ÿ

**å»ºè®®ä¿ç•™**ï¼Œå¦‚æœé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š
- Agent å¾ªç¯è°ƒç”¨å·¥å…·
- Agent åœ¨ä¸è¯¥ä½¿ç”¨å·¥å…·çš„åœºæ™¯ä¸‹è°ƒç”¨å·¥å…·ï¼ˆå¦‚é—®å€™è¯­ï¼‰
- Agent å¯¹åŒä¸€ä¸ªé—®é¢˜å¤šæ¬¡è°ƒç”¨å·¥å…·
- å¸Œæœ›ä¸¥æ ¼æ§åˆ¶ Agent çš„è¡Œä¸º

#### ä½•æ—¶å¯ä»¥å»æ‰ System Promptï¼Ÿ

å¯ä»¥è€ƒè™‘å»æ‰ï¼Œå¦‚æœï¼š
- å¸Œæœ› Agent æœ‰æ›´çµæ´»çš„è¡Œä¸º
- å…è®¸å¤šæ¬¡å·¥å…·è°ƒç”¨ï¼ˆå¤æ‚ä»»åŠ¡éœ€è¦å¤šæ­¥éª¤ï¼‰
- ä½¿ç”¨é»˜è®¤çš„ Agent è¡Œä¸ºå·²ç»æ»¡è¶³éœ€æ±‚

**ç»“è®º**ï¼š
- âœ… **æŠ€æœ¯ä¸Šå¯è¡Œ**ï¼šå»æ‰ `system_prompt` ä¹Ÿå¯ä»¥è¿è¡Œ
- âš ï¸ **åŠŸèƒ½å¯èƒ½å—å½±å“**ï¼šå¯èƒ½æ¢å¤å¾ªç¯è°ƒç”¨å·¥å…·ç­‰é—®é¢˜
- ğŸ’¡ **å»ºè®®ä¿ç•™**ï¼šå½“å‰çš„ `system_prompt` è§£å†³äº†ä¹‹å‰çš„å·¥å…·è°ƒç”¨é—®é¢˜

### JSONæ ¼å¼é”™è¯¯
å¦‚æœé‡åˆ° `400 Bad Request` æˆ– JSON æ ¼å¼é”™è¯¯ï¼š
- ç¡®ä¿ä½¿ç”¨**è‹±æ–‡å¼•å·**ï¼Œä¸è¦ä½¿ç”¨ä¸­æ–‡å¼•å·
- æ£€æŸ¥JSONæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œä¾‹å¦‚ï¼š`'{"message": "ä½ å¥½"}'`
- æŸ¥çœ‹é”™è¯¯å“åº”ä¸­çš„è¯¦ç»†æç¤ºå’Œç¤ºä¾‹
- å¯ä»¥ä½¿ç”¨æ–‡ä»¶æ–¹å¼å‘é€è¯·æ±‚é¿å…è½¬ä¹‰é—®é¢˜ï¼š
  ```bash
  echo '{"message": "ä½ å¥½"}' | curl -X POST http://localhost:8000/chat \
    -H "Content-Type: application/json" \
    -d @- | jq .
  ```

### ç«¯å£å†²çª
å¦‚æœç«¯å£è¢«å ç”¨ï¼š
- **8000 ç«¯å£ï¼ˆChat æœåŠ¡å™¨ï¼‰**ï¼šåœ¨ `docker-compose.yml` ä¸­ä¿®æ”¹ `chat-server` çš„ç«¯å£æ˜ å°„
- **8100 ç«¯å£ï¼ˆFastMCP æœåŠ¡å™¨ï¼‰**ï¼šåœ¨ `docker-compose.yml` ä¸­ä¿®æ”¹ `mcp-server` çš„ç«¯å£æ˜ å°„ï¼Œå¹¶æ›´æ–° `chat_server.py` ä¸­çš„ `MCP_SERVER_URL` ç¯å¢ƒå˜é‡

### FastMCP æœåŠ¡å™¨è¿æ¥å¤±è´¥
å¦‚æœ Chat æœåŠ¡å™¨æ— æ³•è¿æ¥åˆ° FastMCP æœåŠ¡å™¨ï¼š
1. ç¡®ä¿ FastMCP æœåŠ¡å™¨å·²å¯åŠ¨ï¼ˆ`mcp-server` æœåŠ¡ï¼‰
2. æ£€æŸ¥ `MCP_SERVER_URL` ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®ï¼ˆDocker å†…éƒ¨ä½¿ç”¨ `http://mcp-server:8100`ï¼Œæœ¬åœ°ä½¿ç”¨ `http://localhost:8100`ï¼‰
3. æŸ¥çœ‹æ—¥å¿—ç¡®è®¤ä¸¤ä¸ªæœåŠ¡éƒ½åœ¨è¿è¡Œ
4. Chat æœåŠ¡å™¨å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨é‡è¯•è¿æ¥ï¼ˆæœ€å¤š15æ¬¡ï¼Œæ¯æ¬¡é—´éš”2ç§’ï¼‰

### Agent è¿­ä»£æ¬¡æ•°è¾¾åˆ°ä¸Šé™
å¦‚æœé‡åˆ° `Max iterations of 10 reached!` é”™è¯¯ï¼š
1. **æ­£å¸¸ç°è±¡**ï¼šè¡¨ç¤º Agent åœ¨10æ¬¡è¿­ä»£å†…æ— æ³•å®Œæˆä»»åŠ¡
2. **è§£å†³æ–¹æ¡ˆ**ï¼š
   - å°†å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºæ›´ç®€å•çš„æ­¥éª¤
   - é‡æ–°è¡¨è¿°é—®é¢˜ï¼Œä½¿å…¶æ›´æ¸…æ™°æ˜ç¡®
   - æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰è¯¯ï¼ˆå¦‚è¾“å…¥ä¸å®Œæ•´ï¼‰
3. **å“åº”æ ¼å¼**ï¼šå³ä½¿è¾¾åˆ°è¿­ä»£ä¸Šé™ï¼Œä¹Ÿä¼šè¿”å›å‹å¥½çš„é”™è¯¯æç¤º

### æ„å»ºå¤±è´¥
å¦‚æœ Docker æ„å»ºå¤±è´¥ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ç¡®è®¤ä»£ç†é…ç½®æ­£ç¡®
3. å°è¯•æ¸…ç† Docker ç¼“å­˜ï¼š`docker system prune -a`
4. ä½¿ç”¨ `--no-cache` é‡æ–°æ„å»º

## MCP åè®®äº¤äº’åˆ†ææŠ¥å‘Š

æœ¬èŠ‚åŸºäºå®é™…çš„æ—¥å¿—è¾“å‡ºï¼Œè¯¦ç»†åˆ†æä¸€æ¬¡å®Œæ•´çš„ MCP åè®®äº¤äº’æµç¨‹ï¼Œå¸®åŠ©ç†è§£ chat-server å’Œ mcp-server ä¹‹é—´çš„é€šä¿¡è¿‡ç¨‹ã€‚

### è¯·æ±‚åœºæ™¯

ç”¨æˆ·å‘ chat-server å‘é€è¯·æ±‚ï¼š`è®¡ç®— 10 + 20 * 2`

### å®Œæ•´çš„äº¤äº’æµç¨‹

#### 1. SSE è¿æ¥å»ºç«‹

**chat-server â†’ mcp-server**

```
chat-server-1  | 2025-11-03 04:18:30,522 - httpx - INFO - HTTP Request: GET http://mcp-server:8100/sse "HTTP/1.1 200 OK"
```

**mcp-server æ—¥å¿—ï¼š**

```
mcp-server-1   | 2025-11-03 04:18:30,520 - mcp.server.sse - DEBUG - Setting up SSE connection
mcp-server-1   | 2025-11-03 04:18:30,520 - mcp.server.sse - DEBUG - Created new session with ID: bdd2d4d9-feb5-4331-8891-f4a747248ee1
mcp-server-1   | 2025-11-03 04:18:30,521 - mcp.server.sse - DEBUG - Starting SSE response task
mcp-server-1   | INFO:     172.21.0.3:39074 - "GET /sse HTTP/1.1" 200 OK
```

**è¯´æ˜**ï¼šchat-server é€šè¿‡ GET è¯·æ±‚å»ºç«‹ SSE é•¿è¿æ¥ï¼Œmcp-server åˆ›å»ºæ–°çš„ä¼šè¯å¹¶è¿”å›ä¼šè¯ IDã€‚

---

#### 2. åˆå§‹åŒ–é˜¶æ®µï¼ˆinitializeï¼‰

**chat-server â†’ mcp-server**ï¼ˆPOST /messages/ï¼‰

```
chat-server-1  | 2025-11-03 04:18:30,524 - httpx - INFO - HTTP Request: POST http://mcp-server:8100/messages/?session_id=bdd2d4d9feb543318891f4a747248ee1 "HTTP/1.1 202 Accepted"
```

**mcp-server æ—¥å¿— - æ¥æ”¶è¯·æ±‚ï¼š**

```
mcp-server-1   | 2025-11-03 04:18:30,524 - mcp.server.sse - DEBUG - Handling POST message
mcp-server-1   | 2025-11-03 04:18:30,524 - mcp.server.sse - DEBUG - Parsed session ID: bdd2d4d9-feb5-4331-8891-f4a747248ee1
mcp-server-1   | 2025-11-03 04:18:30,524 - mcp.server.sse - DEBUG - Received JSON: b'{"method":"initialize","params":{"protocolVersion":"2025-06-18","capabilities":{},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
mcp-server-1   | 2025-11-03 04:18:30,524 - mcp.server.sse - DEBUG - Validated client message: root=JSONRPCRequest(method='initialize', params={'protocolVersion': '2025-06-18', 'capabilities': {}, 'clientInfo': {'name': 'mcp', 'version': '0.1.0'}}, jsonrpc='2.0', id=0)
```

**mcp-server â†’ chat-server**ï¼ˆé€šè¿‡ SSE æµè¿”å›å“åº”ï¼‰

```
mcp-server-1   | 2025-11-03 04:18:30,524 - mcp.server.sse - DEBUG - Sending message via SSE: SessionMessage(message=JSONRPCMessage(root=JSONRPCResponse(jsonrpc='2.0', id=0, result={'protocolVersion': '2025-06-18', 'capabilities': {...}, 'serverInfo': {'name': 'MathTools', 'version': '1.20.0'}})), metadata=None)
```

**æ ¼å¼åŒ–åçš„å“åº”ï¼ˆé€šè¿‡æ—¥å¿—æ ¼å¼åŒ–å™¨ï¼‰ï¼š**

```
mcp-server-1   | 2025-11-03 04:18:30,524 - sse_starlette.sse - DEBUG - [SSE Chunk - å·²æ ¼å¼åŒ–]
mcp-server-1   | {
mcp-server-1   |   "jsonrpc": "2.0",
mcp-server-1   |   "id": 0,
mcp-server-1   |   "result": {
mcp-server-1   |     "protocolVersion": "2025-06-18",
mcp-server-1   |     "capabilities": {
mcp-server-1   |       "experimental": {},
mcp-server-1   |       "prompts": {"listChanged": false},
mcp-server-1   |       "resources": {"subscribe": false, "listChanged": false},
mcp-server-1   |       "tools": {"listChanged": false}
mcp-server-1   |     },
mcp-server-1   |     "serverInfo": {
mcp-server-1   |       "name": "MathTools",
mcp-server-1   |       "version": "1.20.0"
mcp-server-1   |     }
mcp-server-1   |   }
mcp-server-1   | }
```

**è¯´æ˜**ï¼š
- chat-server å‘é€ `initialize` è¯·æ±‚ï¼ŒåŒ…å«åè®®ç‰ˆæœ¬å’Œå®¢æˆ·ç«¯ä¿¡æ¯
- mcp-server è¿”å›æœåŠ¡å™¨èƒ½åŠ›ä¿¡æ¯å’ŒæœåŠ¡å™¨ä¿¡æ¯ï¼ˆåç§°ã€ç‰ˆæœ¬ï¼‰

---

#### 3. åˆå§‹åŒ–å®Œæˆé€šçŸ¥ï¼ˆnotifications/initializedï¼‰

**chat-server â†’ mcp-server**

```
mcp-server-1   | 2025-11-03 04:18:30,526 - mcp.server.sse - DEBUG - Received JSON: b'{"method":"notifications/initialized","jsonrpc":"2.0"}'
mcp-server-1   | 2025-11-03 04:18:30,526 - mcp.server.sse - DEBUG - Validated client message: root=JSONRPCNotification(method='notifications/initialized', params=None, jsonrpc='2.0')
```

**è¯´æ˜**ï¼šchat-server é€šçŸ¥ mcp-server åˆå§‹åŒ–å·²å®Œæˆï¼ˆè¿™æ˜¯ MCP åè®®çš„æ ‡å‡†æµç¨‹ï¼‰ã€‚

---

#### 4. å·¥å…·åˆ—è¡¨æŸ¥è¯¢ï¼ˆtools/listï¼‰

**chat-server â†’ mcp-server**

```
mcp-server-1   | 2025-11-03 04:18:30,530 - mcp.server.sse - DEBUG - Received JSON: b'{"method":"tools/list","jsonrpc":"2.0","id":1}'
mcp-server-1   | 2025-11-03 04:18:30,530 - mcp.server.sse - DEBUG - Validated client message: root=JSONRPCRequest(method='tools/list', params=None, jsonrpc='2.0', id=1)
```

**mcp-server å¤„ç†è¯·æ±‚ï¼š**

```
mcp-server-1   | 2025-11-03 04:18:30,530 - mcp.server.lowlevel.server - INFO - Processing request of type ListToolsRequest
mcp-server-1   | 2025-11-03 04:18:30,530 - mcp.server.lowlevel.server - DEBUG - Dispatching request of type ListToolsRequest
mcp-server-1   | 2025-11-03 04:18:30,530 - mcp.server.lowlevel.server - DEBUG - Response sent
```

**mcp-server â†’ chat-server**ï¼ˆè¿”å›å·¥å…·åˆ—è¡¨ï¼‰

```
mcp-server-1   | 2025-11-03 04:18:30,531 - mcp.server.sse - DEBUG - Sending message via SSE: SessionMessage(message=JSONRPCMessage(root=JSONRPCResponse(jsonrpc='2.0', id=1, result={'tools': [...]})))
```

**æ ¼å¼åŒ–åçš„å·¥å…·åˆ—è¡¨å“åº”ï¼š**

```
mcp-server-1   | {
mcp-server-1   |   "jsonrpc": "2.0",
mcp-server-1   |   "id": 1,
mcp-server-1   |   "result": {
mcp-server-1   |     "tools": [
mcp-server-1   |       {
mcp-server-1   |         "name": "add_numbers",
mcp-server-1   |         "description": "è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„åŠ æ³•ã€‚\né‡è¦ï¼šä»…åœ¨ç”¨æˆ·æ˜ç¡®è¦æ±‚è¿›è¡ŒåŠ æ³•è®¡ç®—æ—¶ä½¿ç”¨æ­¤å·¥å…·...",
mcp-server-1   |         "inputSchema": {...},
mcp-server-1   |         "outputSchema": {...}
mcp-server-1   |       },
mcp-server-1   |       {
mcp-server-1   |         "name": "multiply_numbers",
mcp-server-1   |         "description": "è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„ä¹˜æ³•ã€‚\né‡è¦ï¼šä»…åœ¨ç”¨æˆ·æ˜ç¡®è¦æ±‚è¿›è¡Œä¹˜æ³•è®¡ç®—æ—¶ä½¿ç”¨æ­¤å·¥å…·...",
mcp-server-1   |         "inputSchema": {...},
mcp-server-1   |         "outputSchema": {...}
mcp-server-1   |       },
mcp-server-1   |       {
mcp-server-1   |         "name": "calculate_expression",
mcp-server-1   |         "description": "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚è¡¨è¾¾å¼å¿…é¡»åªåŒ…å«æ•°å­—å’ŒåŸºæœ¬è¿ç®—ç¬¦...",
mcp-server-1   |         "inputSchema": {...},
mcp-server-1   |         "outputSchema": {...}
mcp-server-1   |       }
mcp-server-1   |     ]
mcp-server-1   |   }
mcp-server-1   | }
```

**è¯´æ˜**ï¼š
- chat-server è¯·æ±‚å·¥å…·åˆ—è¡¨
- mcp-server è¿”å›æ‰€æœ‰å¯ç”¨å·¥å…·åŠå…¶æè¿°ã€è¾“å…¥è¾“å‡ºæ¨¡å¼
- å·¥å…·æè¿°åŒ…å«ä½¿ç”¨åœºæ™¯è¯´æ˜ï¼Œå¸®åŠ© Agent å†³å®šä½•æ—¶ä½¿ç”¨å“ªä¸ªå·¥å…·

---

#### 5. å·¥å…·è°ƒç”¨ï¼ˆtools/callï¼‰

**chat-server â†’ mcp-server**ï¼ˆè¯·æ±‚è°ƒç”¨ `calculate_expression` å·¥å…·ï¼‰

```
mcp-server-1   | 2025-11-03 04:18:30,527 - mcp.server.sse - DEBUG - Received JSON: b'{"method":"tools/call","params":{"name":"calculate_expression","arguments":{"expression":"10 + 20 * 2"}},"jsonrpc":"2.0","id":1}'
mcp-server-1   | 2025-11-03 04:18:30,527 - mcp.server.sse - DEBUG - Validated client message: root=JSONRPCRequest(method='tools/call', params={'name': 'calculate_expression', 'arguments': {'expression': '10 + 20 * 2'}}, jsonrpc='2.0', id=1)
```

**mcp-server å¤„ç†å·¥å…·è°ƒç”¨ï¼š**

```
mcp-server-1   | 2025-11-03 04:18:30,528 - mcp.server.lowlevel.server - INFO - Processing request of type CallToolRequest
mcp-server-1   | 2025-11-03 04:18:30,528 - mcp.server.lowlevel.server - DEBUG - Dispatching request of type CallToolRequest
mcp-server-1   | 2025-11-03 04:18:30,528 - __main__ - INFO - [FastMCP Tool] calculate_expression(expression='10 + 20 * 2')
mcp-server-1   | 2025-11-03 04:18:30,528 - __main__ - INFO - [FastMCP Tool] calculate_expression ç»“æœ: 50.0
mcp-server-1   | 2025-11-03 04:18:30,529 - mcp.server.lowlevel.server - DEBUG - Response sent
```

**mcp-server â†’ chat-server**ï¼ˆè¿”å›å·¥å…·æ‰§è¡Œç»“æœï¼‰

```
mcp-server-1   | 2025-11-03 04:18:30,529 - mcp.server.sse - DEBUG - Sending message via SSE: SessionMessage(message=JSONRPCMessage(root=JSONRPCResponse(jsonrpc='2.0', id=1, result={'content': [{'type': 'text', 'text': '50.0'}], 'structuredContent': {'result': 50.0}, 'isError': False})), metadata=None)
```

**æ ¼å¼åŒ–åçš„å·¥å…·è°ƒç”¨å“åº”ï¼š**

```
mcp-server-1   | {
mcp-server-1   |   "jsonrpc": "2.0",
mcp-server-1   |   "id": 1,
mcp-server-1   |   "result": {
mcp-server-1   |     "content": [
mcp-server-1   |       {
mcp-server-1   |         "type": "text",
mcp-server-1   |         "text": "50.0"
mcp-server-1   |       }
mcp-server-1   |     ],
mcp-server-1   |     "structuredContent": {
mcp-server-1   |       "result": 50.0
mcp-server-1   |     },
mcp-server-1   |     "isError": false
mcp-server-1   |   }
mcp-server-1   | }
```

**è¯´æ˜**ï¼š
- chat-serverï¼ˆAgentï¼‰æ ¹æ®ç”¨æˆ·è¯·æ±‚å†³å®šè°ƒç”¨ `calculate_expression` å·¥å…·ï¼Œå‚æ•°ä¸º `"10 + 20 * 2"`
- mcp-server æ‰§è¡Œå·¥å…·å‡½æ•°ï¼Œè®¡ç®—ç»“æœä¸º `50.0`
- mcp-server è¿”å›æ ¼å¼åŒ–çš„ç»“æœï¼ŒåŒ…å«æ–‡æœ¬æ ¼å¼å’Œç»“æ„åŒ–æ ¼å¼

---

#### 6. æœ€ç»ˆå“åº”

**chat-server è¿”å›ç»™ç”¨æˆ·ï¼š**

```json
{
  "raw_response": "50.0\n\n\n\n## Step 1: Determine the task\n...",
  "tools_available": [
    "add_numbers",
    "multiply_numbers",
    "calculate_expression"
  ]
}
```

---

### å…³é”®å‘ç°

1. **åŒå‘é€šä¿¡**ï¼š
   - chat-server â†’ mcp-serverï¼šé€šè¿‡ `POST /messages/` å‘é€ JSON-RPC è¯·æ±‚
   - mcp-server â†’ chat-serverï¼šé€šè¿‡ SSE æµè¿”å› JSON-RPC å“åº”

2. **ä¼šè¯ç®¡ç†**ï¼š
   - æ¯ä¸ª SSE è¿æ¥æœ‰å”¯ä¸€çš„ `session_id`
   - æ‰€æœ‰è¯·æ±‚éƒ½é€šè¿‡ `?session_id=xxx` å‚æ•°å…³è”åˆ°åŒä¸€ä¸ªä¼šè¯

3. **è¯·æ±‚-å“åº”åŒ¹é…**ï¼š
   - JSON-RPC åè®®é€šè¿‡ `id` å­—æ®µåŒ¹é…è¯·æ±‚å’Œå“åº”
   - ä¾‹å¦‚ï¼šåˆå§‹åŒ–è¯·æ±‚ `id=0`ï¼Œå¯¹åº”çš„å“åº”ä¹Ÿæ˜¯ `id=0`

4. **å·¥å…·è°ƒç”¨æµç¨‹**ï¼š
   - Agent å…ˆè·å–å·¥å…·åˆ—è¡¨ï¼ˆäº†è§£å¯ç”¨å·¥å…·ï¼‰
   - Agent åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œå†³å®šè°ƒç”¨å“ªä¸ªå·¥å…·
   - Agent å‘é€ `tools/call` è¯·æ±‚ï¼ŒåŒ…å«å·¥å…·åç§°å’Œå‚æ•°
   - mcp-server æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
   - Agent å°†ç»“æœæ•´åˆåˆ°æœ€ç»ˆå›å¤ä¸­

5. **æ—¥å¿—æ ¼å¼åŒ–å™¨çš„ä½œç”¨**ï¼š
   - è‡ªåŠ¨æ ¼å¼åŒ– JSON-RPC æ¶ˆæ¯ï¼Œä½¿å…¶æ›´æ˜“è¯»
   - å°†ä¸­æ–‡ Unicode è½¬ä¹‰åºåˆ—è½¬æ¢ä¸ºå¯è¯»ä¸­æ–‡
   - æ ¼å¼åŒ– SSE chunk ä¸­çš„ JSON æ•°æ®
   - å¸®åŠ©å¼€å‘è€…ç†è§£ MCP åè®®çš„è¯¦ç»†äº¤äº’è¿‡ç¨‹

### æ—¥å¿—æŸ¥çœ‹æŠ€å·§

1. **æŸ¥çœ‹å®Œæ•´çš„ MCP äº¤äº’**ï¼šåœ¨ mcp-server æ—¥å¿—ä¸­æœç´¢ `Sending message via SSE` æˆ– `Received JSON`
2. **æŸ¥çœ‹å·¥å…·æ‰§è¡Œ**ï¼šæœç´¢ `[FastMCP Tool]` æŸ¥çœ‹å·¥å…·çš„å®é™…æ‰§è¡Œæƒ…å†µ
3. **æŸ¥çœ‹æ ¼å¼åŒ–åçš„ JSON**ï¼šæœç´¢ `[SSE Chunk - å·²æ ¼å¼åŒ–]` æŸ¥çœ‹æ ¼å¼åŒ–åçš„ JSON å“åº”
4. **è·Ÿè¸ªä¼šè¯**ï¼šé€šè¿‡ `session_id` è·Ÿè¸ªåŒä¸€ä¼šè¯çš„æ‰€æœ‰è¯·æ±‚å’Œå“åº”

### åè®®æµç¨‹å›¾

```
ç”¨æˆ·è¯·æ±‚: "è®¡ç®— 10 + 20 * 2"
    â†“
chat-server (Agent)
    â†“
1. GET /sse (å»ºç«‹ SSE è¿æ¥)
    â†“
2. POST /messages/ (initialize)
    â†“
3. POST /messages/ (notifications/initialized)
    â†“
4. POST /messages/ (tools/list) â† mcp-server è¿”å›å·¥å…·åˆ—è¡¨
    â†“
5. Agent åˆ†æï¼šéœ€è¦è°ƒç”¨ calculate_expression
    â†“
6. POST /messages/ (tools/call) â† mcp-server æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
    â†“
7. Agent ç”Ÿæˆæœ€ç»ˆå›å¤
    â†“
è¿”å›ç»™ç”¨æˆ·: {"raw_response": "...", "tools_available": [...]}
```

---

**æ³¨æ„**ï¼šä»¥ä¸Šæ—¥å¿—åŸºäº DEBUG çº§åˆ«çš„æ—¥å¿—è¾“å‡ºã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒMCP ç›¸å…³æ—¥å¿—å·²è®¾ç½®ä¸º DEBUG çº§åˆ«ï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨è¿›è¡Œæ ¼å¼åŒ–ï¼Œç¡®ä¿ JSON æ•°æ®ä»¥æ˜“è¯»æ ¼å¼è¾“å‡ºï¼Œä¸­æ–‡æ­£ç¡®æ˜¾ç¤ºã€‚

## è°ƒè¯•å’Œå®¹å™¨å‘½ä»¤

### è¿›å…¥å®¹å™¨è¿›è¡Œè°ƒè¯•

å½“éœ€è¦æ·±å…¥è°ƒè¯•æˆ–æ’æŸ¥é—®é¢˜æ—¶ï¼Œå¯ä»¥è¿›å…¥å®¹å™¨æ‰§è¡Œå‘½ä»¤ã€‚

#### æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨

```bash
docker ps
```

#### è¿›å…¥ mcp-server å®¹å™¨

```bash
docker exec -it fastmcp_demo-mcp-server-1 /bin/bash
```

#### è¿›å…¥ chat-server å®¹å™¨

```bash
docker exec -it fastmcp_demo-chat-server-1 /bin/bash
```

### å¸¸ç”¨è°ƒè¯•å‘½ä»¤

#### 1. æ£€æŸ¥ FastMCP å®ä¾‹å±æ€§

åœ¨ mcp-server å®¹å™¨ä¸­æ£€æŸ¥ FastMCP å®ä¾‹çš„ç»“æ„ï¼š

```bash
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "
from mcp.server.fastmcp import FastMCP
mcp = FastMCP('Test')
print('app:', hasattr(mcp, 'app'))
print('_app:', hasattr(mcp, '_app'))
print('sse_app:', hasattr(mcp, 'sse_app'))
print('streamable_http_app:', hasattr(mcp, 'streamable_http_app'))
print('åŒ…å« app çš„å±æ€§:', [x for x in dir(mcp) if 'app' in x.lower()])
"
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
app: False
_app: False
sse_app: True
streamable_http_app: True
åŒ…å« app çš„å±æ€§: ['sse_app', 'streamable_http_app']
```

#### 2. æ£€æŸ¥ Python ç¯å¢ƒå’Œä¾èµ–

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python --version

# æ£€æŸ¥å·²å®‰è£…çš„åŒ…
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/pip list

# æ£€æŸ¥ç‰¹å®šåŒ…
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/pip show fastmcp
```

#### 3. æ£€æŸ¥æ—¥å¿—é…ç½®

```bash
# åœ¨å®¹å™¨å†…æµ‹è¯•æ—¥å¿—æ ¼å¼åŒ–å™¨
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "
import logging
import json
from mcp_server import MCPProtocolFormatter

formatter = MCPProtocolFormatter('%(message)s')
record = logging.LogRecord(
    name='test',
    level=logging.DEBUG,
    pathname='',
    lineno=0,
    msg='Test JSON: %s',
    args=('{\"key\": \"value\"}',),
    exc_info=None
)
print(formatter.format(record))
"
```

#### 4. æ£€æŸ¥ MCP å·¥å…·æ³¨å†Œ

```bash
# æ£€æŸ¥å·¥å…·æ˜¯å¦æ­£ç¡®æ³¨å†Œ
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "
from mcp_server import mcp
print('FastMCP å®ä¾‹:', mcp)
print('å·¥å…·æ•°é‡:', len([x for x in dir(mcp) if not x.startswith('_')]))
"
```

#### 5. æµ‹è¯•å·¥å…·å‡½æ•°

```bash
# ç›´æ¥æµ‹è¯•å·¥å…·å‡½æ•°
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "
from mcp_server import calculate_expression
result = calculate_expression('10 + 20 * 2')
print('è®¡ç®—ç»“æœ:', result)
"
```

#### 6. æ£€æŸ¥ç½‘ç»œè¿æ¥

```bash
# ä» chat-server å®¹å™¨æµ‹è¯•è¿æ¥åˆ° mcp-server
docker exec fastmcp_demo-chat-server-1 /bin/bash -c "
curl -v http://mcp-server:8100/sse 2>&1 | head -20
"

# ä» mcp-server å®¹å™¨æµ‹è¯•è‡ªèº«
docker exec fastmcp_demo-mcp-server-1 /bin/bash -c "
curl -v http://localhost:8100/sse 2>&1 | head -20
"
```

#### 7. æŸ¥çœ‹å®æ—¶æ—¥å¿—

```bash
# æŸ¥çœ‹ mcp-server æ—¥å¿—
docker logs -f fastmcp_demo-mcp-server-1

# æŸ¥çœ‹ chat-server æ—¥å¿—
docker logs -f fastmcp_demo-chat-server-1

# åŒæ—¶æŸ¥çœ‹ä¸¤ä¸ªæœåŠ¡çš„æ—¥å¿—
docker-compose logs -f
```

#### 8. æ£€æŸ¥ç¯å¢ƒå˜é‡

```bash
# æŸ¥çœ‹ mcp-server ç¯å¢ƒå˜é‡
docker exec fastmcp_demo-mcp-server-1 env

# æŸ¥çœ‹ chat-server ç¯å¢ƒå˜é‡
docker exec fastmcp_demo-chat-server-1 env
```

#### 9. æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿ

```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
docker exec fastmcp_demo-chat-server-1 ls -lh /app/models/

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
docker exec fastmcp_demo-mcp-server-1 ls -la /app/.venv/bin/ | head -20

# æ£€æŸ¥ä»£ç æ–‡ä»¶
docker exec fastmcp_demo-mcp-server-1 cat /app/mcp_server.py | head -50
```

### è°ƒè¯•æŠ€å·§

1. **ä½¿ç”¨äº¤äº’å¼ Python Shell**ï¼š
   ```bash
   docker exec -it fastmcp_demo-mcp-server-1 /app/.venv/bin/python
   ```
   ç„¶ååœ¨ Python shell ä¸­å¯¼å…¥æ¨¡å—è¿›è¡Œäº¤äº’å¼è°ƒè¯•ï¼š
   ```python
   >>> from mcp_server import mcp
   >>> import inspect
   >>> print(inspect.getmembers(mcp))
   ```

2. **ä¿®æ”¹ä»£ç å¹¶é‡æ–°åŠ è½½**ï¼š
   - å¦‚æœä½¿ç”¨ Docker volumes æŒ‚è½½ä»£ç ï¼Œä¿®æ”¹åå®¹å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å˜åŒ–ï¼ˆå¦‚æœä½¿ç”¨å¼€å‘æ¨¡å¼ï¼‰
   - æˆ–è€…éœ€è¦é‡å¯å®¹å™¨ï¼š`docker-compose restart mcp-server`

3. **å¯ç”¨æ›´è¯¦ç»†çš„æ—¥å¿—**ï¼š
   - åœ¨å®¹å™¨å†…ä¿®æ”¹æ—¥å¿—çº§åˆ«æˆ–æ·»åŠ ä¸´æ—¶æ—¥å¿—è¯­å¥
   - æŸ¥çœ‹æ ¼å¼åŒ–åçš„æ—¥å¿—è¾“å‡º

4. **ç½‘ç»œè°ƒè¯•**ï¼š
   - ä½¿ç”¨ `curl` æˆ– `wget` æµ‹è¯• HTTP ç«¯ç‚¹
   - æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾ï¼š`netstat -tlnp`ï¼ˆå¦‚æœå¯ç”¨ï¼‰

### å¸¸è§é—®é¢˜æ’æŸ¥

#### é—®é¢˜ï¼šå®¹å™¨æ— æ³•å¯åŠ¨

```bash
# æŸ¥çœ‹å®¹å™¨å¯åŠ¨æ—¥å¿—
docker logs fastmcp_demo-mcp-server-1

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps -a | grep fastmcp_demo
```

#### é—®é¢˜ï¼šæ¨¡å—å¯¼å…¥é”™è¯¯

```bash
# æ£€æŸ¥ Python è·¯å¾„
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "import sys; print(sys.path)"

# æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ä»¥å¯¼å…¥
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "import mcp_server; print('å¯¼å…¥æˆåŠŸ')"
```

#### é—®é¢˜ï¼šå·¥å…·è°ƒç”¨å¤±è´¥

```bash
# ç›´æ¥æµ‹è¯•å·¥å…·å‡½æ•°
docker exec fastmcp_demo-mcp-server-1 /app/.venv/bin/python -c "
from mcp_server import add_numbers, multiply_numbers, calculate_expression
print('add_numbers(2, 3):', add_numbers(2, 3))
print('multiply_numbers(4, 5):', multiply_numbers(4, 5))
print('calculate_expression(\"10+20*2\"):', calculate_expression('10+20*2'))
"
```

---

**æç¤º**ï¼šä»¥ä¸Šå‘½ä»¤å¯ä»¥å¸®åŠ©å¿«é€Ÿå®šä½é—®é¢˜ã€‚å¦‚æœé‡åˆ°æ— æ³•è§£å†³çš„é—®é¢˜ï¼Œå¯ä»¥æŸ¥çœ‹å®Œæ•´çš„å®¹å™¨æ—¥å¿—æˆ–è”ç³»ç»´æŠ¤è€…ã€‚
