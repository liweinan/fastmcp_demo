# FastMCP Demo 环境变量配置

# MCP服务器URL（Docker内部使用服务名，本地使用localhost）
MCP_SERVER_URL=http://localhost:8100

# LLM模型配置
# 仅支持Llama 3.1 8B-Instruct（支持原生tool_calls）
# 模型文件路径（相对于项目根目录）
LLAMA_MODEL_PATH=./models/llama-3.1-8b-instruct-q4_k_m.gguf

# 代理配置（可选，用于 Docker 构建时）
# build.sh 会自动读取这些环境变量并传递给 Docker 构建
# 可以设置以下任意一个（优先级：PROXY_URL > HTTP_PROXY > http_proxy）
# PROXY_URL=http://your-proxy:port
# HTTP_PROXY=http://your-proxy:port
# HTTPS_PROXY=http://your-proxy:port
# http_proxy=http://your-proxy:port
# https_proxy=http://your-proxy:port
# NO_PROXY=localhost,127.0.0.1
#
# 或者直接设置 BUILD_PROXY（将直接传递给 Docker 构建）：
# BUILD_PROXY=http://your-proxy:port
#
# 注意：如果代理在 localhost，build.sh 会自动转换为 host.docker.internal